<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>RAndom NoTes - Paper Club</title><link href="http://blog.bshi.engineer/" rel="alternate"></link><link href="http://blog.bshi.engineer/feeds/paper-club.atom.xml" rel="self"></link><id>http://blog.bshi.engineer/</id><updated>2017-09-27T22:24:00-04:00</updated><entry><title>Hidden Technical Debt in Machine Learning Systems</title><link href="http://blog.bshi.engineer/weekly-paper-club-003-ml-technical-debt.html" rel="alternate"></link><published>2017-09-27T00:00:00-04:00</published><updated>2017-09-27T22:24:00-04:00</updated><author><name>Dash Shi</name></author><id>tag:blog.bshi.engineer,2017-09-27:/weekly-paper-club-003-ml-technical-debt.html</id><summary type="html">&lt;p&gt;This paper discuss the ML-specific technical debts that real world systems are facing.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I found this paper through HN and it is an old &lt;a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf"&gt;NIPS paper&lt;/a&gt; published in 2015 by a list of Googlers. I'll try to summarize all the problems this paper mentioned that real-world ML systems are facing.&lt;/p&gt;
&lt;h4&gt;Decouple features and make strict abstractions&lt;/h4&gt;
&lt;h5&gt;Entanglement&lt;/h5&gt;
&lt;p&gt;Machine Learning models are usually trying to learn how to generate a prediction &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; with the given features &lt;span class="math"&gt;\(x_1,\ldots,x_n\)&lt;/span&gt;. If any of the feature &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; changes, then all the learned parameters in this model will needs to be adjusted accordingly. Adding or deleting new features cause the same problem. This also applies to adjusting hyper-parameters, optimizers, and other tweaks. They call this the &lt;strong&gt;CACE&lt;/strong&gt; principle. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;CACE: Changing Anything Changes Everything.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A possible solution is to separate a large model into several models and train an ensemble model. But the reason that ensemble models work well is because the errors of individual models are uncorrelated. Therefore if one model get changed and gets slightly better results but now its error correlated with another model in the ensemble system, then the overall performance may get worse.&lt;/p&gt;
&lt;p&gt;They also mentioned another way of deteching changes using some high-dimensional visualization tool but I haven't checked it yet (Ad click
prediction: a view from the trenches.).&lt;/p&gt;
&lt;h5&gt;Correction Cascades&lt;/h5&gt;
&lt;p&gt;In production systems the ML models may depends on each other while working on similar cases. For example &lt;span class="math"&gt;\(m_a\)&lt;/span&gt; is trained to solve task &lt;span class="math"&gt;\(a\)&lt;/span&gt;, and &lt;span class="math"&gt;\(a^\prime\)&lt;/span&gt; is a slightly different task so we train a &lt;span class="math"&gt;\(m_a^\prime\)&lt;/span&gt; based on &lt;span class="math"&gt;\(m_a\)&lt;/span&gt; to solve it, and this can also applies to &lt;span class="math"&gt;\(a^{\prime\prime}\)&lt;/span&gt; and so on.
In such case it is hard to monitor the improvement of each individual model. One way of solving this is feeding a variable to &lt;span class="math"&gt;\(m_a\)&lt;/span&gt; and let it learn how to distinguish these similar tasks.&lt;/p&gt;
&lt;p&gt;However I'm not sure this could solve the entanglement problem because now this model is still vulnerable to CACE.&lt;/p&gt;
&lt;h5&gt;Undeclared Customer&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Make sure you have ACL on the produced features so you know who are using it and your changes will not break their system.&lt;/li&gt;
&lt;li&gt;When depending on data without ownership or data that may change dynamically, do versioning to avoid input characteristic changes.&lt;/li&gt;
&lt;li&gt;Spend time to identiy features you used in the system, remove out-dated features, correlated features, and features that only marginall increase the performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;This is an unfinished post.&lt;/strong&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="research"></category><category term="paper club"></category><category term="machine learning"></category></entry><entry><title>Learning Edge Representations via Low-Rank Asymmetric Projections</title><link href="http://blog.bshi.engineer/weekly-paper-club-002-edge-representation.html" rel="alternate"></link><published>2017-09-24T00:00:00-04:00</published><updated>2017-09-24T16:24:00-04:00</updated><author><name>Dash Shi</name></author><id>tag:blog.bshi.engineer,2017-09-24:/weekly-paper-club-002-edge-representation.html</id><summary type="html">&lt;p&gt;This paper uses random walk with DNN to generate an asymmetric edge score for two given nodes and then train the model using a loss similar to logistic regression with sampled negative labels.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This paper is published at CIKM'17 and can be found at &lt;a href="https://arxiv.org/pdf/1705.05615.pdf"&gt;Arxiv&lt;/a&gt; and the code is available at &lt;a href="http://sami.haija.org/graph/deep_embedding.html"&gt;here&lt;/a&gt; however as of Sept 24th it is showing &lt;code&gt;Coming Soon!&lt;/code&gt;. The corresponding author of this work is the author of &lt;a href="https://arxiv.org/abs/1403.6652"&gt;DeepWalk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Most network representation models learn representations &lt;span class="math"&gt;\(Y_u\)&lt;/span&gt; for each node &lt;span class="math"&gt;\(u\)&lt;/span&gt; in the graph and measure the closeness/edge betwee two nodes &lt;span class="math"&gt;\(u\)&lt;/span&gt; and &lt;span class="math"&gt;\(v\)&lt;/span&gt; by some distance measurement &lt;span class="math"&gt;\(\mathsf{dist}(Y_u, Y_v)\)&lt;/span&gt;. In this paper they call this method as &lt;strong&gt;node-centric&lt;/strong&gt; and has two drawbacks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Such method implicitly assumes the connections are undirected, which means &lt;span class="math"&gt;\(u\rightarrow v\)&lt;/span&gt; and &lt;span class="math"&gt;\(v\rightarrow u\)&lt;/span&gt; are equivalent because they are sharing the same distance function,&lt;/li&gt;
&lt;li&gt;The authors claim that in some cases, the representation matrix &lt;span class="math"&gt;\(|V|\times d\)&lt;/span&gt; would be larger than the sparse adjacent matrix which has &lt;span class="math"&gt;\(|E|\)&lt;/span&gt; elements.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this work the authors propose a method to learn the node embeddings as follow:&lt;/p&gt;
&lt;p&gt;For each node &lt;span class="math"&gt;\(u\)&lt;/span&gt; in the graph, they learn an embedding &lt;span class="math"&gt;\(Y_u \in \mathbb{R}^D\)&lt;/span&gt;. Then they pass it through a shared deep neural network denoted by &lt;span class="math"&gt;\(f:\mathbb{R}^D\rightarrow \mathbb{R}^d\)&lt;/span&gt; to reduce the dimension from &lt;span class="math"&gt;\(D\)&lt;/span&gt; to &lt;span class="math"&gt;\(d\)&lt;/span&gt;. They also create an edge likelihood function &lt;span class="math"&gt;\(g(u,v):f(Y_u)^T\times M \times f(Y_v)\)&lt;/span&gt; to represent the edges. The matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt; is a low-rank matrix which is defined as &lt;span class="math"&gt;\(M = L \times R\)&lt;/span&gt;, &lt;span class="math"&gt;\(L = \mathbb{R}^{d\times b}\)&lt;/span&gt;, &lt;span class="math"&gt;\(R = \mathbb{R}^{b\times d}\)&lt;/span&gt;. The edge representation is a combination of two vectors, the source embedding &lt;span class="math"&gt;\(Rf(Y_v)\)&lt;/span&gt; and the destination embedding &lt;span class="math"&gt;\(L^Tf(Y_u)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Model Illustration" src="images/paper_club_002_edge_rep.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Although one could use the trained &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional embeddings for inference, this model still needs to learn two &lt;span class="math"&gt;\(|V|\times D\)&lt;/span&gt; embedding matrices during training like other models with an extra DNN.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The DNN in this work is a two layer fully-connected layers with batch normalization and uses relu as the activation function.&lt;/p&gt;
&lt;p&gt;Like word2vec, in this work the model also learns two representations &lt;span class="math"&gt;\(Y_u^{\textsf{source}}\)&lt;/span&gt; and &lt;span class="math"&gt;\(Y_u^{\textsf{dest}}\)&lt;/span&gt; for a single node &lt;span class="math"&gt;\(u\)&lt;/span&gt;. Then the edge likelihood of &lt;span class="math"&gt;\((u,v)\)&lt;/span&gt; and &lt;span class="math"&gt;\((v,u)\)&lt;/span&gt; would be &lt;span class="math"&gt;\(\textsf{dist}(Y_u^{\textsf{source}},Y_v^{\textsf{dest}})\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textsf{dist}(Y_v^{\textsf{source}},Y_u^{\textsf{dest}})\)&lt;/span&gt; respectively.&lt;/p&gt;
&lt;p&gt;The difference between this work and their previous DeepWalk are threefold:&lt;/p&gt;
&lt;h3&gt;The Graph Likelihood Function&lt;/h3&gt;
&lt;p&gt;Instead of minimizing the usual loss function used in other random walk based network representation models which is usually defined as&lt;/p&gt;
&lt;div class="math"&gt;$$\underset{\theta}{\mathrm{argmin}}\prod_{v\in V}\prod_{c \in N(v)} p(c|v;\theta)$$&lt;/div&gt;
&lt;p&gt;in which &lt;span class="math"&gt;\(N(v)\)&lt;/span&gt; is the neighborhood node set of node &lt;span class="math"&gt;\(u\)&lt;/span&gt; defined by a random walk path and a contex window size (you could find the details in DeepWalk).&lt;/p&gt;
&lt;p&gt;This work defines a new loss function based on the idea of logistic regression&lt;/p&gt;
&lt;div class="math"&gt;$$\Pr(G) \propto \prod_{u\in V, v\in V} \sigma(g(u,v))^{\mathcal{D}_{u,v}}(1-\sigma(g(u,v)))^{\mathbb{I}((u,v)\notin E_{train}}$$&lt;/div&gt;
&lt;p&gt;in which the first term &lt;span class="math"&gt;\(\prod_{u\in V, v\in V} \sigma(g(u,v))^{\mathcal{D}_{u,v}}\)&lt;/span&gt; is enabled when &lt;span class="math"&gt;\(u\)&lt;/span&gt; and &lt;span class="math"&gt;\(v\)&lt;/span&gt; co-occur in the same context window by the unnormalized requency &lt;span class="math"&gt;\(\mathcal{D}_{u,v}\)&lt;/span&gt;, and &lt;span class="math"&gt;\((1-\sigma(g(u,v)))^{\mathbb{I}((u,v)\notin E_{train}}\)&lt;/span&gt; is enabled when &lt;span class="math"&gt;\(u\rightarrow v\)&lt;/span&gt; is a false edge. Note that the authors did not specify if &lt;span class="math"&gt;\((u,v)\notin E_{train}\)&lt;/span&gt; only considers direct connection or multi-hop connection.&lt;/p&gt;
&lt;p&gt;Either way, their final loss function is more straightforward &lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L} = \mathbb{E}_{(u,v) \sim \mathcal{D}/Z}\left[\log\sigma(g(u,v)) + \sum_{v^-\in Sample(K, u\hat{-})}\log(1-\sigma(g(u,v^-)))\right]$$&lt;/div&gt;
&lt;p&gt;By explicitly selecting true node pairs and false node pairs, one no longer needs to worry about the aforementioned problem in the &lt;span class="math"&gt;\(\Pr(G)\)&lt;/span&gt; equation.&lt;/p&gt;
&lt;h3&gt;The Edge Likelihood Function&lt;/h3&gt;
&lt;p&gt;This is the &lt;span class="math"&gt;\(g(\cdot)\)&lt;/span&gt; we discussed previously.&lt;/p&gt;
&lt;h3&gt;The Context Definition&lt;/h3&gt;
&lt;p&gt;The context is defined differently for undirected graphs and directed graphs. For paths in undirected graphs, context of &lt;span class="math"&gt;\(u_i\)&lt;/span&gt; in &lt;span class="math"&gt;\(u_1\rightarrow\cdots\rightarrow u_i\rightarrow\cdots\rightarrow u_k\)&lt;/span&gt; are &lt;span class="math"&gt;\(\{u_1,\cdots,u_{i-1},\cdots,u_{k}\}\)&lt;/span&gt;; whereas in directed graphs, the context of &lt;span class="math"&gt;\(u_i\)&lt;/span&gt; becomes &lt;span class="math"&gt;\(\{u_{i+1},\cdots,u_k\}\)&lt;/span&gt;. But I'm not sure why this would improve the performance because the previous nodes should also play a role when defining the embedding.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="research"></category><category term="deep learning"></category><category term="paper club"></category><category term="representation learning"></category></entry><entry><title>StarSpace, embed anything!</title><link href="http://blog.bshi.engineer/weekly-paper-club-001-network-embedding.html" rel="alternate"></link><published>2017-09-21T00:00:00-04:00</published><updated>2017-09-21T10:24:00-04:00</updated><author><name>Dash Shi</name></author><id>tag:blog.bshi.engineer,2017-09-21:/weekly-paper-club-001-network-embedding.html</id><summary type="html">&lt;p&gt;StarSpace is a general representation learning model that learn entity embeddings using feature embeddings.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a new paper submitted to AAAI'18 by Facebook AI Research (FAIR) and the full article can be found on &lt;a href="https://arxiv.org/abs/1709.03856"&gt;Arxiv&lt;/a&gt;, the code is available at &lt;a href="https://github.com/facebookresearch/Starspace"&gt;GitHub&lt;/a&gt;. One of the author &lt;a href="https://research.fb.com/people/bordes-antoine/"&gt;Antoine Bordes&lt;/a&gt; is also the author of TransE, a well-known Knowledge Graph Completion model.&lt;/p&gt;
&lt;p&gt;In general, StarSpace is capable of learning embeddings from a set of finite features. If we have a finite feature dictionary &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; and has a feature matrix &lt;span class="math"&gt;\(F \in \mathbb{R}^{\mathcal{D}\times d}\)&lt;/span&gt;, in which &lt;span class="math"&gt;\(F_i\)&lt;/span&gt; is the &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional embedding of &lt;span class="math"&gt;\(i^{\mathsf{th}}\)&lt;/span&gt; feature. Then for an entity &lt;span class="math"&gt;\(a\)&lt;/span&gt; with k features &lt;span class="math"&gt;\(\mathbf{a}=\{i_1,\cdots,i_k\}\)&lt;/span&gt;, the embedding of entity &lt;span class="math"&gt;\(a\)&lt;/span&gt; is defined as unweighted sum of all these feature embeddings &lt;span class="math"&gt;\(\sum_{i\in\mathbf{a}}F_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;When training the model, StarSpace uses &lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{(a,b)\in E^+, b^- \in E^-} L^{\textsf{batch}}(sim(a,b),sim(a,b^-_1),\ldots,sim(a,b^-_k))$$&lt;/div&gt;
&lt;p&gt;in which&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(E^+\)&lt;/span&gt; is the positive entity pair set.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(E^-\)&lt;/span&gt; is the negative pair set that &lt;span class="math"&gt;\(\{(a,b) | (a,\cdot) \in E^+, (a,b) \notin E^+ \}\)&lt;/span&gt;, which means &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; has the correct left-hand and right-hand entity type but &lt;span class="math"&gt;\(b\)&lt;/span&gt; is not the correct right-hand element. For example if we are predicting LocatedIn, then &lt;span class="math"&gt;\(a\)&lt;/span&gt; will be a building and &lt;span class="math"&gt;\(b\)&lt;/span&gt; will be a location.&lt;/li&gt;
&lt;li&gt;The sim function, according to the authors, is best to be inner product for smaller number of features (&lt;span class="math"&gt;\(|\mathbf{a}|\)&lt;/span&gt;) and cosine for larger number of features.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; is better to be ranking loss instead of softmax loss. (What is the ranking loss in this case? It should not be the standard pairwise ranking loss I guess)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is relatively easy to do the multi-class classification because the labels can be viewed as &lt;span class="math"&gt;\(b\)&lt;/span&gt;, same thing applies to word/sentence/document similarity in which each pair of similar word/sentence/document can be represented by &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt;. As for Knowledge Graph Completion, &lt;span class="math"&gt;\(a\)&lt;/span&gt; could be h &amp;amp; r and &lt;span class="math"&gt;\(b\)&lt;/span&gt; be t or &lt;span class="math"&gt;\(a\)&lt;/span&gt; be h and &lt;span class="math"&gt;\(b\)&lt;/span&gt; be r &amp;amp; t.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="research"></category><category term="deep learning"></category><category term="paper club"></category><category term="representation learning"></category><category term="network representation"></category></entry></feed>