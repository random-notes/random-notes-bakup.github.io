<!DOCTYPE html>
<html lang="en">
<head>
<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-106957224-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments)};
		  gtag('js', new Date());

			  gtag('config', 'UA-106957224-1');
				</script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>RAndom NoTes | articles tagged "paper club"</title>
    <link rel="shortcut icon" type="image/png" href="http://blog.bshi.engineer/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="http://blog.bshi.engineer/favicon.ico">
    <link href="http://blog.bshi.engineer/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="RAndom NoTes Full Atom Feed" />
    <link rel="stylesheet" href="http://blog.bshi.engineer/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="http://blog.bshi.engineer/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="http://blog.bshi.engineer/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Dash Shi" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li class="ephemeral selected"><a href="http://blog.bshi.engineer/tag/paper-club.html">paper club</a></li>
                <li><a href="http://blog.bshi.engineer/">Home</a></li>
                <li><a href="http://blog.bshi.engineer/tags">Tags</a></li>
                <li><a href="http://blog.bshi.engineer/categories">Categories</a></li>
                <li><a href="http://blog.bshi.engineer/archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="http://blog.bshi.engineer/">RAndom NoTes</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Dec 05, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.bshi.engineer/weekly-paper-club-004-weighted-average-sentence-embedding.html" rel="bookmark" title="Permanent Link to &quot;A Simple But Tough-To-Beat Baseline For Sentence Embedding&quot;">A Simple But Tough-To-Beat Baseline For Sentence Embedding</a>
                </h2>

                
                

                <p>You can find the paper at <a href="https://openreview.net/pdf?id=SyK00v5xx">ICLR'17</a>.</p>
<p><strong>TL;DR:</strong> Word2Vec(CBOW) actually is a weighted average model, using weighted word average as sentence embedding can achieve a good result.</p>
<p>This paper is based on a latent variable generative process which is </p>
<div class="math">$$\mathsf{Pr}[w\ \textsf{emitted at time}\ t | c_t] \propto \exp(&lt; c_t, v_w&gt;),$$</div>
<p>in which view the probability of word <span class="math">\(w\)</span> appears at time <span class="math">\(t\)</span> in a sentence given discourse/context vector <span class="math">\(c_t\)</span> is proportional to <span class="math">\(e^{c_t \cdot v_w}\)</span>.</p>
<p>The authors claim that the discourse vector <span class="math">\(c_t\)</span> is based on some <em>slow random walk process</em>, they further rewrite the latent variable generative model to</p>
<div class="math">$$\mathsf{Pr}[w\ \textsf{emitted at time}\ t | c_t] = \alpha p(w) + (1-\alpha)\frac{\exp(&lt;\widetilde{c}_s,v_w&gt;)}{Z_{\widetilde{c}_s}},$$</div>
<p>in which <span class="math">\(\widetilde{c}_s=\beta c_0 + (1-\beta)c_s, c_0 \perp c_s\)</span>, and <span class="math">\(Z_{\widetilde{c}_s} = \Sigma_{w\in V}\exp(&lt;\widetilde{c}_s, v_w&gt;)\)</span>. This shows that a word <span class="math">\(w\)</span> may have a non-zero probability if </p>
<ol>
<li>it is related to sentence discourse <span class="math">\(c_s\)</span>, or</li>
<li>term <span class="math">\(\alpha p(w) &gt; 0\)</span>, which means it is a frequent word, or</li>
<li><span class="math">\(v_w\)</span> is correlated to <span class="math">\(c_0\)</span>.</li>
</ol>
<p>After a few other equations, the final objective function looks like</p>
<div class="math">$$arg\ \textsf{max}\Sigma_{w\in s}f_w(\widetilde{c}_s)\propto \Sigma_{w\in s}\frac{a}{p(w) + a}v_w,$$</div>
<p>where <span class="math">\(a = \frac{1-a}{\alpha Z}\)</span>.</p>
<p>The algorithm is much simpler than the equations:</p>
<p><img alt="Algorithm of Sentence Embedding" src="images/paper_club_004_algo.png"></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <div class="clear"></div>

                <div class="info">
                    <a href="http://blog.bshi.engineer/weekly-paper-club-004-weighted-average-sentence-embedding.html">posted at 11:34</a>
                    &nbsp;&middot;&nbsp;<a href="http://blog.bshi.engineer/category/paper-club.html" rel="tag">Paper Club</a>
                    &nbsp;&middot;
                    &nbsp;<a href="http://blog.bshi.engineer/tag/research.html" class="tags">research</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/paper-club.html" class="tags selected">paper club</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/machine-learning.html" class="tags">machine learning</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/representation-learning.html" class="tags">representation learning</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/natural-language-processing.html" class="tags">natural language processing</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/sentence-embedding.html" class="tags">sentence embedding</a>
                </div>
		<a href="http://blog.bshi.engineer/weekly-paper-club-004-weighted-average-sentence-embedding.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Sep 24, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.bshi.engineer/weekly-paper-club-002-edge-representation.html" rel="bookmark" title="Permanent Link to &quot;Learning Edge Representations via Low-Rank Asymmetric Projections&quot;">Learning Edge Representations via Low-Rank Asymmetric Projections</a>
                </h2>

                
                

                <p>This paper is published at CIKM'17 and can be found at <a href="https://arxiv.org/pdf/1705.05615.pdf">Arxiv</a> and the code is available at <a href="http://sami.haija.org/graph/deep_embedding.html">here</a> however as of Sept 24th it is showing <code>Coming Soon!</code>. The corresponding author of this work is the author of <a href="https://arxiv.org/abs/1403.6652">DeepWalk</a>.</p>
<p>Most network representation models learn representations <span class="math">\(Y_u\)</span> for each node <span class="math">\(u\)</span> in the graph and measure the closeness/edge betwee two nodes <span class="math">\(u\)</span> and <span class="math">\(v\)</span> by some distance measurement <span class="math">\(\mathsf{dist}(Y_u, Y_v)\)</span>. In this paper they call this method as <strong>node-centric</strong> and has two drawbacks:</p>
<ol>
<li>Such method implicitly assumes the connections are undirected, which means <span class="math">\(u\rightarrow v\)</span> and <span class="math">\(v\rightarrow u\)</span> are equivalent because they are sharing the same distance function,</li>
<li>The authors claim that in some cases, the representation matrix <span class="math">\(|V|\times d\)</span> would be larger than the sparse adjacent matrix which has <span class="math">\(|E|\)</span> elements.</li>
</ol>
<p>In this work the authors propose a method to learn the node embeddings as follow:</p>
<p>For each node <span class="math">\(u\)</span> in the graph, they learn an embedding <span class="math">\(Y_u \in \mathbb{R}^D\)</span>. Then they pass it through a shared deep neural network denoted by <span class="math">\(f:\mathbb{R}^D\rightarrow \mathbb{R}^d\)</span> to reduce the dimension from <span class="math">\(D\)</span> to <span class="math">\(d\)</span>. They also create an edge likelihood function <span class="math">\(g(u,v):f(Y_u)^T\times M \times f(Y_v)\)</span> to represent the edges. The matrix <span class="math">\(M\)</span> is a low-rank matrix which is defined as <span class="math">\(M = L \times R\)</span>, <span class="math">\(L = \mathbb{R}^{d\times b}\)</span>, <span class="math">\(R = \mathbb{R}^{b\times d}\)</span>. The edge representation is a combination of two vectors, the source embedding <span class="math">\(Rf(Y_v)\)</span> and the destination embedding <span class="math">\(L^Tf(Y_u)\)</span>.</p>
<p><img alt="Model Illustration" src="images/paper_club_002_edge_rep.png"></p>
<p><em>Although one could use the trained <span class="math">\(d\)</span>-dimensional embeddings for inference, this model still needs to learn two <span class="math">\(|V|\times D\)</span> embedding matrices during training like other models with an extra DNN.</em></p>
<p>The DNN in this work is a two layer fully-connected layers with batch normalization and uses relu as the activation function.</p>
<p>Like word2vec, in this work the model also learns two representations <span class="math">\(Y_u^{\textsf{source}}\)</span> and <span class="math">\(Y_u^{\textsf{dest}}\)</span> for a single node <span class="math">\(u\)</span>. Then the edge likelihood of <span class="math">\((u,v)\)</span> and <span class="math">\((v,u)\)</span> would be <span class="math">\(\textsf{dist}(Y_u^{\textsf{source}},Y_v^{\textsf{dest}})\)</span> and <span class="math">\(\textsf{dist}(Y_v^{\textsf{source}},Y_u^{\textsf{dest}})\)</span> respectively.</p>
<p>The difference between this work and their previous DeepWalk are threefold:</p>
<h3>The Graph Likelihood Function</h3>
<p>Instead of minimizing the usual loss function used in other random walk based network representation models which is usually defined as</p>
<div class="math">$$\underset{\theta}{\mathrm{argmin}}\prod_{v\in V}\prod_{c \in N(v)} p(c|v;\theta)$$</div>
<p>in which <span class="math">\(N(v)\)</span> is the neighborhood node set of node <span class="math">\(u\)</span> defined by a random walk path and a contex window size (you could find the details in DeepWalk).</p>
<p>This work defines a new loss function based on the idea of logistic regression</p>
<div class="math">$$\Pr(G) \propto \prod_{u\in V, v\in V} \sigma(g(u,v))^{\mathcal{D}_{u,v}}(1-\sigma(g(u,v)))^{\mathbb{I}((u,v)\notin E_{train}}$$</div>
<p>in which the first term <span class="math">\(\prod_{u\in V, v\in V} \sigma(g(u,v))^{\mathcal{D}_{u,v}}\)</span> is enabled when <span class="math">\(u\)</span> and <span class="math">\(v\)</span> co-occur in the same context window by the unnormalized requency <span class="math">\(\mathcal{D}_{u,v}\)</span>, and <span class="math">\((1-\sigma(g(u,v)))^{\mathbb{I}((u,v)\notin E_{train}}\)</span> is enabled when <span class="math">\(u\rightarrow v\)</span> is a false edge. Note that the authors did not specify if <span class="math">\((u,v)\notin E_{train}\)</span> only considers direct connection or multi-hop connection.</p>
<p>Either way, their final loss function is more straightforward </p>
<div class="math">$$\mathcal{L} = \mathbb{E}_{(u,v) \sim \mathcal{D}/Z}\left[\log\sigma(g(u,v)) + \sum_{v^-\in Sample(K, u\hat{-})}\log(1-\sigma(g(u,v^-)))\right]$$</div>
<p>By explicitly selecting true node pairs and false node pairs, one no longer needs to worry about the aforementioned problem in the <span class="math">\(\Pr(G)\)</span> equation.</p>
<h3>The Edge Likelihood Function</h3>
<p>This is the <span class="math">\(g(\cdot)\)</span> we discussed previously.</p>
<h3>The Context Definition</h3>
<p>The context is defined differently for undirected graphs and directed graphs. For paths in undirected graphs, context of <span class="math">\(u_i\)</span> in <span class="math">\(u_1\rightarrow\cdots\rightarrow u_i\rightarrow\cdots\rightarrow u_k\)</span> are <span class="math">\(\{u_1,\cdots,u_{i-1},\cdots,u_{k}\}\)</span>; whereas in directed graphs, the context of <span class="math">\(u_i\)</span> becomes <span class="math">\(\{u_{i+1},\cdots,u_k\}\)</span>. But I'm not sure why this would improve the performance because the previous nodes should also play a role when defining the embedding.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <div class="clear"></div>

                <div class="info">
                    <a href="http://blog.bshi.engineer/weekly-paper-club-002-edge-representation.html">posted at 00:00</a>
                    &nbsp;&middot;&nbsp;<a href="http://blog.bshi.engineer/category/paper-club.html" rel="tag">Paper Club</a>
                    &nbsp;&middot;
                    &nbsp;<a href="http://blog.bshi.engineer/tag/research.html" class="tags">research</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/deep-learning.html" class="tags">deep learning</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/paper-club.html" class="tags selected">paper club</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/representation-learning.html" class="tags">representation learning</a>
                </div>
		<a href="http://blog.bshi.engineer/weekly-paper-club-002-edge-representation.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Sep 21, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.bshi.engineer/weekly-paper-club-001-network-embedding.html" rel="bookmark" title="Permanent Link to &quot;StarSpace, embed anything!&quot;">StarSpace, embed anything!</a>
                </h2>

                
                

                <p>This is a new paper submitted to AAAI'18 by Facebook AI Research (FAIR) and the full article can be found on <a href="https://arxiv.org/abs/1709.03856">Arxiv</a>, the code is available at <a href="https://github.com/facebookresearch/Starspace">GitHub</a>. One of the author <a href="https://research.fb.com/people/bordes-antoine/">Antoine Bordes</a> is also the author of TransE, a well-known Knowledge Graph Completion model.</p>
<p>In general, StarSpace is capable of learning embeddings from a set of finite features. If we have a finite feature dictionary <span class="math">\(\mathcal{D}\)</span> and has a feature matrix <span class="math">\(F \in \mathbb{R}^{\mathcal{D}\times d}\)</span>, in which <span class="math">\(F_i\)</span> is the <span class="math">\(d\)</span>-dimensional embedding of <span class="math">\(i^{\mathsf{th}}\)</span> feature. Then for an entity <span class="math">\(a\)</span> with k features <span class="math">\(\mathbf{a}=\{i_1,\cdots,i_k\}\)</span>, the embedding of entity <span class="math">\(a\)</span> is defined as unweighted sum of all these feature embeddings <span class="math">\(\sum_{i\in\mathbf{a}}F_i\)</span>.</p>
<p>When training the model, StarSpace uses </p>
<div class="math">$$\sum_{(a,b)\in E^+, b^- \in E^-} L^{\textsf{batch}}(sim(a,b),sim(a,b^-_1),\ldots,sim(a,b^-_k))$$</div>
<p>in which</p>
<ul>
<li><span class="math">\(E^+\)</span> is the positive entity pair set.</li>
<li><span class="math">\(E^-\)</span> is the negative pair set that <span class="math">\(\{(a,b) | (a,\cdot) \in E^+, (a,b) \notin E^+ \}\)</span>, which means <span class="math">\(a\)</span> and <span class="math">\(b\)</span> has the correct left-hand and right-hand entity type but <span class="math">\(b\)</span> is not the correct right-hand element. For example if we are predicting LocatedIn, then <span class="math">\(a\)</span> will be a building and <span class="math">\(b\)</span> will be a location.</li>
<li>The sim function, according to the authors, is best to be inner product for smaller number of features (<span class="math">\(|\mathbf{a}|\)</span>) and cosine for larger number of features.</li>
<li><span class="math">\(\mathcal{L}\)</span> is better to be ranking loss instead of softmax loss. (What is the ranking loss in this case? It should not be the standard pairwise ranking loss I guess)</li>
</ul>
<p>It is relatively easy to do the multi-class classification because the labels can be viewed as <span class="math">\(b\)</span>, same thing applies to word/sentence/document similarity in which each pair of similar word/sentence/document can be represented by <span class="math">\(a\)</span> and <span class="math">\(b\)</span>. As for Knowledge Graph Completion, <span class="math">\(a\)</span> could be h &amp; r and <span class="math">\(b\)</span> be t or <span class="math">\(a\)</span> be h and <span class="math">\(b\)</span> be r &amp; t.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <div class="clear"></div>

                <div class="info">
                    <a href="http://blog.bshi.engineer/weekly-paper-club-001-network-embedding.html">posted at 00:00</a>
                    &nbsp;&middot;&nbsp;<a href="http://blog.bshi.engineer/category/paper-club.html" rel="tag">Paper Club</a>
                    &nbsp;&middot;
                    &nbsp;<a href="http://blog.bshi.engineer/tag/research.html" class="tags">research</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/deep-learning.html" class="tags">deep learning</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/paper-club.html" class="tags selected">paper club</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/representation-learning.html" class="tags">representation learning</a>
                    &nbsp;<a href="http://blog.bshi.engineer/tag/network-representation.html" class="tags">network representation</a>
                </div>
		<a href="http://blog.bshi.engineer/weekly-paper-club-001-network-embedding.html#disqus_thread">Click to read and post comments</a>
            </article>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="http://blog.bshi.engineer/feeds/all.atom.xml" rel="alternate">Atom Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>